{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d0ebc6",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643fcbd",
   "metadata": {},
   "source": [
    "## load logits data (EHR model and MAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52228e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466 466 466\n"
     ]
    }
   ],
   "source": [
    "outs_dir = \"response/mimic-iv_outcome_test_RETAIN_llama-3.3-70b-instruct_PubMed_MSD/agent_logits_outs.pkl\"\n",
    "\n",
    "with open(outs_dir, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "preds = data[\"preds\"]\n",
    "labels = data[\"labels\"]\n",
    "models = data[\"model\"]\n",
    "\n",
    "with open(val_outs_dir, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "val_preds = data[\"preds\"]\n",
    "val_labels = data[\"labels\"]\n",
    "val_models = data[\"model\"]\n",
    "\n",
    "print(len(preds), len(labels), len(models))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0b86c",
   "metadata": {},
   "source": [
    "## Metrics_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08de0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/qsong1/daling.shi/.conda/envs/colacare/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchmetrics import AUROC, Accuracy, AveragePrecision\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "import numpy as np\n",
    "from sklearn import metrics as sklearn_metrics\n",
    "\n",
    "\n",
    "def minpse(preds, labels):\n",
    "    precisions, recalls, _ = sklearn_metrics.precision_recall_curve(labels, preds)\n",
    "    minpse_score = np.max([min(x, y) for (x, y) in zip(precisions, recalls)])\n",
    "    return minpse_score\n",
    "\n",
    "\n",
    "def get_binary_metrics(preds, labels):\n",
    "    preds = torch.tensor(preds, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    accuracy = Accuracy(task=\"binary\", threshold=0.5)\n",
    "    auroc = AUROC(task=\"binary\")\n",
    "    auprc = AveragePrecision(task=\"binary\")\n",
    "    f1 = BinaryF1Score()\n",
    "\n",
    "    # convert labels type to int\n",
    "    labels = labels.type(torch.int)\n",
    "    accuracy(preds, labels)\n",
    "    auroc(preds, labels)\n",
    "    auprc(preds, labels)\n",
    "    f1(preds, labels)\n",
    "\n",
    "    # return a dictionary\n",
    "    return {\n",
    "        \"auprc\": auprc.compute().item(),\n",
    "        \"auroc\": auroc.compute().item(),\n",
    "        \"minpse\": minpse(preds, labels),\n",
    "        \"accuracy\": accuracy.compute().item(),\n",
    "        \"f1\": f1.compute().item(),\n",
    "    }\n",
    "\n",
    "\n",
    "def bootstrap(preds, labels, K=100, seed=42):\n",
    "    \"\"\"Bootstrap resampling for binary classification metrics. Resample K times\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n = len(preds)    \n",
    "    # Initialize a list to store bootstrap samples\n",
    "    samples = []\n",
    "    # Create K bootstrap samples\n",
    "    for _ in range(K):\n",
    "        # Sample with replacement from the indices\n",
    "        sample_idx = np.random.choice(n, n, replace=True)\n",
    "        # Get bootstrap sample of preds and labels and store them\n",
    "        samples.append((preds[sample_idx], labels[sample_idx]))\n",
    "    return samples\n",
    "\n",
    "\n",
    "def bootstrap_metrics(samples):\n",
    "    metrics = {k: [] for k in [\"auprc\", \"auroc\", \"minpse\", \"accuracy\", \"f1\"]}\n",
    "    for sample_p, sample_l in samples:\n",
    "        res = get_binary_metrics(sample_p, sample_l)\n",
    "        for k, v in res.items():\n",
    "            metrics[k].append(v)\n",
    "    # calculate mean and std\n",
    "    for k, v in metrics.items():\n",
    "        arr_v = np.array(v)\n",
    "        metrics[k] = {\"mean\": np.mean(arr_v), \"std\": np.std(arr_v)}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def run_bootstrap(preds, labels, K=100, seed=42):\n",
    "    bootstrap_samples = bootstrap(preds, labels, K=K, seed=seed)\n",
    "    metrics = bootstrap_metrics(bootstrap_samples)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1074cf7",
   "metadata": {},
   "source": [
    "## Result (val + CV + bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f814d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 100}\n",
      "\n",
      "▶ Baseline\n",
      "auprc: 54.21 ± 4.13\n",
      "auroc: 63.89 ± 2.94\n",
      "minpse: 55.44 ± 3.40\n",
      "accuracy: 63.34 ± 2.31\n",
      "f1: 45.58 ± 3.59\n",
      "\n",
      "▶ Fusion (baseline + llm)\n",
      "auprc: 54.33 ± 4.13\n",
      "auroc: 64.30 ± 2.88\n",
      "minpse: 55.73 ± 3.34\n",
      "accuracy: 62.89 ± 2.15\n",
      "f1: 47.65 ± 3.41\n",
      "\n",
      "▶ LLM Prob\n",
      "auprc: 54.00 ± 4.01\n",
      "auroc: 64.06 ± 2.79\n",
      "minpse: 54.54 ± 3.44\n",
      "accuracy: 60.71 ± 2.26\n",
      "f1: 56.61 ± 3.11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "p_base = np.array(models)\n",
    "p_llm = np.array(preds)\n",
    "y_true = np.array(labels)\n",
    "\n",
    "val_p_base = np.array(val_models)\n",
    "val_p_llm = np.array(val_preds)\n",
    "val_y_true = np.array(val_labels)\n",
    "\n",
    "# ====== Fusion =======\n",
    "X_train = np.stack([val_p_base, val_p_llm], axis=1)\n",
    "y_train = val_y_true\n",
    "\n",
    "X_test = np.stack([p_base, p_llm], axis=1)\n",
    "y_test = y_true\n",
    "\n",
    "\n",
    "# Step 1: Perform hyperparameter tuning on the train set\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "grid = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "# Step 2: Retrain on the entire train set using the optimal parameters\n",
    "best_model = LogisticRegression(C=grid.best_params_['C'], max_iter=1000)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Perform final evaluation on the test set\n",
    "p_fused = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "def show_metrics(name, preds, labels):\n",
    "    metrics = run_bootstrap(preds, labels)\n",
    "    metrics = {k: f\"{v['mean']*100:.2f} ± {v['std']*100:.2f}\" for k, v in metrics.items()}\n",
    "    print(f\"\\n▶ {name}\")\n",
    "    for k,v in metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "# Baseline metrics\n",
    "show_metrics(\"Baseline\", p_base, y_true)\n",
    "#show_metrics(\"Baseline\", p_base_test, y_test)\n",
    "# Fusion metrics\n",
    "show_metrics(\"Fusion (baseline + llm)\", p_fused, y_test)\n",
    "# LLM prob metrics\n",
    "show_metrics(\"LLM Prob\", p_llm, y_true)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colacare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
